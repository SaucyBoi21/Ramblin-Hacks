{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, LSTM, Masking, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(frames, max_frames):\n",
    "    new_frames = frames\n",
    "    while len(new_frames) < max_frames:\n",
    "        zero_array = np.zeros((224,224, 3))\n",
    "        new_frames.append(zero_array)\n",
    "    \n",
    "    return new_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_resize_frames(video_path, target_size=(224,224), max_frames=120):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    while len(frames) < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "    \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_resized = cv2.resize(frame, target_size)\n",
    "        frame_resized = frame_resized / 255.0\n",
    "        frames.append(frame_resized)\n",
    "    \n",
    "    #while len(frames) < max_frames:\n",
    "    #    frames.append(np.zeros((640, 360)))\n",
    "\n",
    "    frames = padding(frames, 120)\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(frames, max_frames=120):\n",
    "    mask = np.ones((max_frames,))  \n",
    "    if len(frames) < max_frames:\n",
    "        mask[len(frames):] = 0  \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_rows(df, num_repeats=120):\n",
    "    repeated_df = pd.DataFrame(np.tile(df.values, (num_repeats, 1)), columns=df.columns)\n",
    "    \n",
    "    return repeated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(X_folder, y_filepath):\n",
    "    X_filepaths = []\n",
    "    X_arrays = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        X_filename = f\"{X_folder}{i+1}.mp4\"\n",
    "        X_filepaths.append(X_filename)\n",
    "    \n",
    "    for filepath in X_filepaths:\n",
    "        X_array = extract_and_resize_frames(filepath)\n",
    "        X_arrays.append(X_array)\n",
    "\n",
    "    y_df = pd.read_csv(y_filepath)\n",
    "    y_df.drop(columns='filename', axis=1, inplace=True)\n",
    "    y_df = y_df.head(2)\n",
    "    \n",
    "\n",
    "\n",
    "    X_arrays = np.array(X_arrays)   \n",
    "    y_array = y_df.to_numpy()\n",
    "    \n",
    "    return X_arrays, y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = generate_df(\"../model/data/videos/\", \"../model/data/coordinates/coordinate_data.csv\")\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_extraction_cnn(input_shape):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 120, 224, 224, 3), dtype=float32). Expected shape (None, 224, 224, 3), but input has incompatible shape (None, 120, 224, 224, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 120, 224, 224, 3), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m feature_extracter = create_feature_extraction_cnn(input_shape=(\u001b[32m224\u001b[39m,\u001b[32m224\u001b[39m,\u001b[32m3\u001b[39m))\n\u001b[32m      2\u001b[39m feature_extracter.compile(\n\u001b[32m      3\u001b[39m     optimizer = \u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     loss=\u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mfeature_extracter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/virtualenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/virtualenv/lib/python3.11/site-packages/keras/src/models/functional.py:273\u001b[39m, in \u001b[36mFunctional._adjust_input_rank\u001b[39m\u001b[34m(self, flat_inputs)\u001b[39m\n\u001b[32m    271\u001b[39m             adjusted.append(ops.expand_dims(x, axis=-\u001b[32m1\u001b[39m))\n\u001b[32m    272\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    274\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected shape \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    275\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    276\u001b[39m     )\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[31mValueError\u001b[39m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 120, 224, 224, 3), dtype=float32). Expected shape (None, 224, 224, 3), but input has incompatible shape (None, 120, 224, 224, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 120, 224, 224, 3), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "feature_extracter = create_feature_extraction_cnn(input_shape=(224,224,3))\n",
    "feature_extracter.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "feature_extracter.fit(\n",
    "    X, y, epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, X, y):\n",
    "    model = Sequential([\n",
    "        Masking(mask_value=0.0, input_shape = input_shape),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        \n",
    "        Dense(2)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='mse',\n",
    "        metrics = ['mae']\n",
    "    )\n",
    "    \n",
    "    model.fit(X, y, epochs=5)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saahas/.pyenv/versions/3.11.11/envs/virtualenv/lib/python3.11/site-packages/keras/src/layers/core/masking.py:48: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm_13\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 120, 244, 244)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[315]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m244\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m244\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[314]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mbuild_model\u001b[39m\u001b[34m(input_shape, X, y)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_model\u001b[39m(input_shape, X, y):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     model = \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mMasking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     model.compile(\n\u001b[32m     14\u001b[39m         optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     15\u001b[39m         loss=\u001b[33m'\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     16\u001b[39m         metrics = [\u001b[33m'\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     19\u001b[39m     model.fit(X, y, epochs=\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/virtualenv/lib/python3.11/site-packages/keras/src/models/sequential.py:76\u001b[39m, in \u001b[36mSequential.__init__\u001b[39m\u001b[34m(self, layers, trainable, name)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m.add(layer, rebuild=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/virtualenv/lib/python3.11/site-packages/keras/src/models/sequential.py:149\u001b[39m, in \u001b[36mSequential._maybe_rebuild\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers) > \u001b[32m1\u001b[39m:\n\u001b[32m    148\u001b[39m     input_shape = \u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m].batch_shape\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33minput_shape\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers) > \u001b[32m1\u001b[39m:\n\u001b[32m    151\u001b[39m     \u001b[38;5;66;03m# We can build the Sequential model if the first layer has the\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# `input_shape` property. This is most commonly found in Functional\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# model.\u001b[39;00m\n\u001b[32m    154\u001b[39m     input_shape = \u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m].input_shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/virtualenv/lib/python3.11/site-packages/keras/src/layers/layer.py:230\u001b[39m, in \u001b[36mLayer.__new__.<locals>.build_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m obj._open_name_scope():\n\u001b[32m    229\u001b[39m     obj._path = current_path()\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[32m    232\u001b[39m signature = inspect.signature(original_build_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/virtualenv/lib/python3.11/site-packages/keras/src/models/sequential.py:195\u001b[39m, in \u001b[36mSequential.build\u001b[39m\u001b[34m(self, input_shape)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._layers[\u001b[32m1\u001b[39m:]:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[32m    197\u001b[39m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[32m    198\u001b[39m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[32m    199\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/virtualenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/envs/virtualenv/lib/python3.11/site-packages/keras/src/layers/input_spec.py:186\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec.ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spec.allow_last_axis_squeeze:\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ndim != spec.ndim:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    187\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of layer \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    188\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mis incompatible with the layer: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    190\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    191\u001b[39m         )\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec.max_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim > spec.max_ndim:\n",
      "\u001b[31mValueError\u001b[39m: Input 0 of layer \"lstm_13\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 120, 244, 244)"
     ]
    }
   ],
   "source": [
    "model = build_model(input_shape=(120,244,244), X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
